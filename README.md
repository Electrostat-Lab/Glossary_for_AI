# Math Glossary

## Definitions

**Hessian**  
A square matrix of second-order partial derivatives of a scalar-valued function, which is used in optimization to determine the curvature of multivariable functions.

**Random Variable**  
A variable whose possible values are numerical outcomes of a random phenomenon, categorized into discrete (finite or countable outcomes) and continuous (infinite outcomes).

**Matrix**  
A rectangular array of numbers, symbols, or expressions, arranged in rows and columns, used in linear algebra to represent systems of linear equations and perform various matrix operations.

**Entropy**  
In information theory, entropy is a measure of the unpredictability or randomness of a data source, indicating the amount of uncertainty in the next piece of information.

**Mutual Information**  
A measure that quantifies the amount of information obtained about one random variable through another, reflecting the reduction in uncertainty for one variable given a known value of another.

**Dot Product**  
An algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number, used to determine angles between vectors or calculate physical quantities like work.

**Mean**  
The average value of a dataset, calculated as the sum of all data points divided by the number of points, representing the central tendency of the data.

**Variance**  
A statistical measure of the dispersion of data points in a dataset, calculated as the average of the squared differences from the Mean.

**L2 Norm**  
Also known as the Euclidean norm, it represents the distance from the origin to the point in a Euclidean space, calculated as the square root of the sum of the squares of its components.

**Chain Rule (Differentiation)**  
A fundamental calculus rule for finding the derivative of composite functions, stating that the derivative is the product of the derivatives of the composed functions.

**Fourier Transform**  
A mathematical transform that decomposes functions depending on space or time into functions depending on frequency, used extensively in signal analysis and physics.

**Continuity**  
A function is continuous if, at every point in the domain, the limit of the function as it approaches the point equals the function's value at that point.

**Lipschitz Continuity**  
A function satisfies Lipschitz continuity if there exists a constant such that, for all point pairs in its domain, the absolute difference in function values is bounded by the product of the constant and the distance between points.

**Chain Rule (Probability)**  
A probability theory rule that allows the computation of the joint distribution of a set of random variables using only conditional probabilities.

**Polynomial**  
An expression consisting of variables and coefficients, involving only the operations of addition, subtraction, multiplication, and non-negative integer exponentiation.

**Cantor's Diagonal Argument**  
A mathematical proof demonstrating that there are infinite sets which cannot be put into one-to-one correspondence with the infinite set of natural numbers.

**Jacobian**  
A matrix of all first-order partial derivatives of a vector-valued function, important for transformations of coordinates and differential equations.

**Linear Operator**  
A mapping between two vector spaces that preserves the operations of vector addition and scalar multiplication.

**Gradient**  
In vector calculus, the gradient of a scalar function is a vector field that points in the direction of the greatest rate of increase of the function.

**Bayes' Theorem**  
A fundamental theorem in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event.

**Vector**  
An object with both magnitude and direction, used to describe quantities in physics and engineering.

**Joint Law, Product Law**  
The joint law refers to the joint probability distribution of two or more random variables, and the product law is a rule to find the probability that two events both occur.

**Gaussian Distribution**  
Also known as the normal distribution, it's a probability distribution that describes how the values of a variable are distributed. It is the basis for the bell-shaped curve.

**Distribution**  
In statistics, a distribution is a function that shows the possible values for a variable and the frequency of these values.

**Determinant**  
A scalar value that can be computed from the elements of a square matrix and encodes properties like the matrix's invertibility.

**Rank**  
The rank of a matrix is defined as the maximum number of linearly independent row or column vectors in the matrix.

**Eigen-decomposition**  
The process of decomposing a matrix into eigenvalues and eigenvectors, which is crucial for solving systems of linear equations, among other applications.

**SVD**  
Singular value decomposition, a method of decomposing a matrix into three matrices, exposing many of the useful properties of the original matrix.

**Maximum Likelihood**  
A method of estimating the parameters of a statistical model, which maximizes the likelihood that the process described by the model produces the observed data.

**Central Limit Theorem**  
A statistical theory that states when independent random variables are added, their normalized sum tends toward a normal distribution, regardless of the original variables' distribution.

